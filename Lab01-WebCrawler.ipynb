{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pCaNEE5V4Kd"
   },
   "source": [
    "# Lab01: Web Crawler.\n",
    "\n",
    "- MSSV: 19120689\n",
    "- Họ và tên: Lại Khánh Toàn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RW12ABtEV4Kf"
   },
   "source": [
    "## Yêu cầu bài tập\n",
    "\n",
    "**Cách làm bài**\n",
    "\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; trong file, từ `TODO` để cho biết những phần mà bạn cần phải làm.\n",
    "\n",
    "Bạn có thể thảo luận ý tưởng cũng như tham khảo các tài liệu, nhưng *code và bài làm phải là của bạn*. \n",
    "\n",
    "Nếu vi phạm thì sẽ bị 0 điểm cho bài tập này.\n",
    "\n",
    "**Cách nộp bài**\n",
    "\n",
    "Trước khi nộp bài, rerun lại notebook (`Kernel` -> `Restart & Run All`).\n",
    "\n",
    "Sau đó, tạo thư mục có tên `MSSV` của bạn (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) Chép file `Lab01-WebCrawler.ipynb`, `links.txt`, `images.txt` vào, rồi nén thư mục `MSSV` này lại và nộp ở link trên moodle.\n",
    "\n",
    "**Nội dung bài tập**\n",
    "\n",
    "Sử dụng module regex và request để thu thập đường dẫn và hình ảnh từ một trang web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Thu thập dữ liệu đơn giản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXPfWz-V4Kg"
   },
   "source": [
    "### 1.1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZH1daHoV4Kg"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WzzuOjDGV4Kl"
   },
   "source": [
    "### 1.2. Thu thập liên kết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKNjb0THV4Km",
    "outputId": "ef7e8623-d644-4001-ce5c-31b845a37035"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e52ec7eb4e28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#Save urls to links.txt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'links.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'urls' is not defined"
     ]
    }
   ],
   "source": [
    "r = requests.get(\"http://www.hcmus.edu.vn\")\n",
    "\n",
    "# TODO:\n",
    "# Displays all the links present in the specified root URL - \"http://www.hcmus.edu.vn\"\n",
    "# use regex to find all urls from r.text (http[s]://...)\n",
    "# urls = ? \n",
    "\n",
    "\n",
    "# Checking results?\n",
    "# for i in urls:\n",
    "#     print(i)\n",
    "\n",
    "#Save urls to links.txt\n",
    "fp = open('links.txt', 'w')\n",
    "for i in urls:\n",
    "    fp.write(i)\n",
    "    fp.write('\\n')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZJne0sSV4Kq"
   },
   "source": [
    "### 1.3. Thu thập hình ảnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IiYJxbp6V4Kq",
    "outputId": "2b6d5963-213f-4842-c61f-52d94c7b5b43"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Displays the source of all the images present in the root URL:\n",
    "# img_src=?\n",
    "\n",
    "\n",
    "# Checking results?\n",
    "# for i in img_src:\n",
    "#     print(i)\n",
    "\n",
    "fp = open('images.txt', 'w')\n",
    "for i in img_src:\n",
    "    fp.write(i)\n",
    "    fp.write('\\n')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNQ_Q-Ziben-"
   },
   "source": [
    "## Part 2. Thu thập và thể hiện dữ liệu web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeeBnKp2ben-"
   },
   "source": [
    "### 2.1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0A3kkU9ben_"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oW76m9ogben_"
   },
   "source": [
    "### 2.2. HTML Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4NQvzKSbeoA"
   },
   "source": [
    "\n",
    "Bộ phân tích cú pháp HTML (HTML Parser): nhận HTML code và trích xuất thông tin liên quan như tiêu đề của trang, đoạn văn trong trang, tiêu đề trong trang, liên kết, văn bản in đậm, v.v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXZKNmHobeoA"
   },
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        print(\"Start tag:\", tag)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        print(\"End tag :\", tag)\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        print(\"Data  :\", data)\n",
    "\n",
    "parser = MyHTMLParser()\n",
    "parser.feed('<html><head><title>Test</title></head>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hBR89ZYbeoB"
   },
   "source": [
    "#### Loại bỏ các HTML tag không cần thiết bằng cách thiết lập filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUPtUSLIbeoB"
   },
   "outputs": [],
   "source": [
    "def text_filter(element):\n",
    "    if element.parent.name in ['style', 'title', 'script', 'head', '[document]', 'class', 'a', 'li']:\n",
    "        return False\n",
    "    elif isinstance(element, Comment):\n",
    "        '''Opinion mining?'''\n",
    "        return False\n",
    "    elif re.match(r\"[\\s\\r\\n]+\",str(element)): \n",
    "        '''space, return, endline'''\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv_K3YSTbeoB"
   },
   "source": [
    "### 2.3 Thu thập nội dung trang Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0TWTZy2beoC"
   },
   "source": [
    "#### Trong bài tập này ta thể hiện tài liệu (hay nội dung trang web) với cấu trúc đơn giản như sau: \n",
    "- Gọi $D$ là một tập tài liệu chứa *n* tài liệu: $D=\\left\\{d_1,d_2,...,d_n\\right\\}$.\n",
    "- Ta thể hiện tài liệu bằng một dictionary `data={}` với `data[word]=[[url_idx_1,url_idx_2,...],frequency]` với `url_index`$\\in{\\left[{1,n}\\right]}$\n",
    "\n",
    "VD: `data['at']=[[1,2], 5]`\n",
    "Từ `at` xuất hiện trong đường dẫn có index `1` và `2` tổng số lần xuất hiện là 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dmRymoMbeoC"
   },
   "source": [
    "#### Bước 1: liệt kê các từ xuất hiện trong trang web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNsOj9HDbeoC"
   },
   "outputs": [],
   "source": [
    "def wordList(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    text = soup.findAll(text=True)\n",
    "    filtered_text = list(filter(text_filter, text)) # list của các chuỗi\n",
    "    word_list = []\n",
    "\n",
    "    # TODO:\n",
    "    # Với mỗi chuỗi trong filtered_text:\n",
    "    #   Thay thế các dấu câu thành khoảng trắng (gợi ý: danh sách các dấu câu: string.punctuation; thay thế: .replace(...))\n",
    "    #   Tách chuỗi bởi khoảng trắng (.split(...))\n",
    "    #   Thêm các từ vừa được tách ra vào word_list\n",
    "    \n",
    "    return word_list\n",
    "  \n",
    "# Test\n",
    "print(wordList('https://en.wikipedia.org/wiki/Web_mining'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzKscEdtbeoC"
   },
   "source": [
    "#### Bước 2: Tính tần suất xuất hiện của từ trong 1 trang web, lưu trữ dữ liệu vào data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HO-qUDncbeoD"
   },
   "outputs": [],
   "source": [
    "def read_url(url, url_idx, data):\n",
    "    word_list = wordList(url)\n",
    "    # TODO\n",
    "    # Với mỗi từ w trong word_list:\n",
    "    #   Nếu w chưa có trong data thì khởi tạo data[w] = [[url_idx], 1]\n",
    "    #   Ngược lại thì thêm url_idx vào data[w][0] (nếu chưa có) và tăng data[w][1] lên 1 đơn vị\n",
    "    \n",
    "\n",
    "# Test\n",
    "test_data = {}\n",
    "read_url('https://en.wikipedia.org/wiki/Web_mining', 1, test_data)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-paYxxpbeoD"
   },
   "source": [
    "#### Bước 3: Chạy chương trình lưu kết quả vào file output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRNXLKBgbeoD"
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "read_url('https://en.wikipedia.org/wiki/Web_mining', 1, data)\n",
    "read_url('https://en.wikipedia.org/wiki/Data_mining', 2, data)\n",
    "\n",
    "sorted_keys = sorted(data.keys())\n",
    "\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    output_line = \"Word\".ljust(20) + \"Frequency\".ljust(15) + \"URL_idx\".ljust(15) + \"\\n\"\n",
    "    f.writelines(output_line)\n",
    "    f.writelines('---------------------------------------------------------------------\\n\\n')\n",
    "    for key in sorted_keys:\n",
    "        output_string = str(key).ljust(20) + str(data[key][1]).ljust(15) + str(data[key][0]).ljust(15) + \"\\n\"\n",
    "        f.writelines(output_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2l3CRsQ5beoE"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Keyword tìm kiếm: python pickle\n",
    "# Dùng pickle để lưu dictionary data xuống đĩa. Sau đó đọc lên và in ra 10 giá trị đầu tiên của nó.\n",
    "\n",
    "# 1. Lưu data\n",
    "import pickle\n",
    "pickle.dump(...)\n",
    "\n",
    "# 2. Đọc lên và in ra\n",
    "with open(..., \"rb\") as f:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab01-WebCrawler.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
